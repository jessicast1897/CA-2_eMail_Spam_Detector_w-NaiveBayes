{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **CA02 â€“ Email Spam Classification using Naive Bayes**"
      ],
      "metadata": {
        "id": "tgmbNniSscmQ"
      },
      "id": "tgmbNniSscmQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182ada92-37ba-4145-9d0f-7e405b759f84",
      "metadata": {
        "id": "182ada92-37ba-4145-9d0f-7e405b759f84",
        "outputId": "e85d1868-0490-4936-a34d-281b0188718f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model using Multinomial Naive Bayes .....\n",
            "Predicting .....\n",
            "Accuracy: 0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Building the Dictionary (Vocabulary):\n",
        "\n",
        "All training emails are read line by line and split into individual words.\n",
        "To reduce noise in the data, non-alphabetic tokens and single-character words\n",
        "are removed. Word frequencies are then counted across all training emails,\n",
        "and the 3000 most common words are selected to form the dictionary.\n",
        "\n",
        "This dictionary defines the feature space that is used when extracting\n",
        "features from both the training and test datasets.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def make_Dictionary(root_dir, vocab_size=3000):\n",
        "    all_words = []\n",
        "    emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
        "    for mail in emails:\n",
        "        with open(mail, encoding=\"latin-1\") as m:\n",
        "            for line in m:\n",
        "                all_words += line.split()\n",
        "\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    # remove non-alphabetic tokens and 1-letter tokens\n",
        "    for token in list(dictionary.keys()):\n",
        "        if (not token.isalpha()) or (len(token) == 1):\n",
        "            del dictionary[token]\n",
        "\n",
        "    return dictionary.most_common(vocab_size)\n",
        "\n",
        "\"\"\"\n",
        "Feature Extraction:\n",
        "\n",
        "Each email is transformed into a numerical feature vector using a\n",
        "Bag-of-Words representation. Each element in the feature vector\n",
        "corresponds to the number of times a specific dictionary word appears\n",
        "in the email.\n",
        "\n",
        "Email labels are assigned based on file naming conventions:\n",
        "- Files starting with \"spmsg\" are labeled as spam (1)\n",
        "- All other files are labeled as non-spam (0)\n",
        "\n",
        "The first two lines of each email are ignored since the actual\n",
        "email body begins on the third line.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def extract_features(mail_dir, dictionary):\n",
        "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "    features_matrix = np.zeros((len(files), len(dictionary)), dtype=np.float64)\n",
        "    labels = np.zeros(len(files), dtype=np.int64)\n",
        "\n",
        "    # map word -> column index (fast + avoids the \"wordID=0\" bug)\n",
        "    word_to_idx = {word: i for i, (word, _) in enumerate(dictionary)}\n",
        "\n",
        "    for docID, fil in enumerate(files):\n",
        "        with open(fil, encoding=\"latin-1\") as fi:\n",
        "            for line in fi:\n",
        "                for word in line.split():\n",
        "                    idx = word_to_idx.get(word)\n",
        "                    if idx is not None:\n",
        "                        features_matrix[docID, idx] += 1\n",
        "\n",
        "        # label: spam files start with \"spmsg\"\n",
        "        if os.path.basename(fil).startswith(\"spmsg\"):\n",
        "            labels[docID] = 1\n",
        "\n",
        "    return features_matrix, labels\n",
        "\n",
        "# REQUIRED paths for the assignment\n",
        "TRAIN_DIR = r\"C:\\Users\\jessi\\Downloads\\CA2data\\train-mails\"\n",
        "TEST_DIR  = r\"C:\\Users\\jessi\\Downloads\\CA2data\\test-mails\"\n",
        "\n",
        "\n",
        "# Build dictionary from training set only\n",
        "dictionary = make_Dictionary(TRAIN_DIR, vocab_size=3000)\n",
        "\n",
        "# Build feature matrices\n",
        "X_train, y_train = extract_features(TRAIN_DIR, dictionary)\n",
        "X_test, y_test   = extract_features(TEST_DIR, dictionary)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Model Training and Evaluation:\n",
        "\n",
        "The feature matrix generated from the training emails is used to train\n",
        "a Multinomial Naive Bayes classifier. This model is appropriate because\n",
        "the input features represent discrete word count data.\n",
        "\n",
        "Once trained, the model is applied to the test dataset to generate\n",
        "predicted labels. Classification accuracy is then calculated by\n",
        "comparing the predicted labels to the true test labels.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training Model using Multinomial Naive Bayes .....\")\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Predicting .....\")\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dd5bd0-ff13-4e9c-b50d-5466246444de",
      "metadata": {
        "id": "b8dd5bd0-ff13-4e9c-b50d-5466246444de"
      },
      "outputs": [],
      "source": [
        "# chatgpt link https://chatgpt.com/share/e/698a3b8a-67dc-800d-a322-2f3e1fcdffb4"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}